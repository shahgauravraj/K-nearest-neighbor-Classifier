# K-nearest-neighbor-Classifier
K-nearest neighbors algorithm (k-NN) is a non-parametric machine learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. It basically assumes that similar things exist in close proximity i.e., similar things are near to each other. 
We have used this algorithm as a tool to built our own K-nearest neighbor classifier. This piece of program acts as a template to classify any kind of data fed into it. It is suitable for both classification and regression kind of problems.